{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'method testing of deepface'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# torch.device('cuda')\n",
    "detector_backend = 'retinaface'\n",
    "from deepface import DeepFace\n",
    "dfs=DeepFace.represent(\n",
    "    img_path='IMG_1526.JPG',\n",
    "    detector_backend=detector_backend,\n",
    "    model_name=\"Facenet512\"\n",
    ")\n",
    "\"\"\"method testing of deepface\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'embedding': [0.06450958549976349,\n",
       "   2.03857421875,\n",
       "   -0.3013954758644104,\n",
       "   1.5968587398529053,\n",
       "   0.8961449265480042,\n",
       "   -1.4984787702560425,\n",
       "   0.2988607883453369,\n",
       "   1.842509388923645,\n",
       "   1.2629249095916748,\n",
       "   -1.4060503244400024,\n",
       "   0.6606496572494507,\n",
       "   -1.0619127750396729,\n",
       "   0.3211910128593445,\n",
       "   0.5580234527587891,\n",
       "   -0.39439526200294495,\n",
       "   -1.2198841571807861,\n",
       "   -0.7892335653305054,\n",
       "   2.2197580337524414,\n",
       "   -0.6737608909606934,\n",
       "   -0.021783601492643356,\n",
       "   0.9778566956520081,\n",
       "   -0.249063178896904,\n",
       "   0.3419552147388458,\n",
       "   0.5114160776138306,\n",
       "   0.24859803915023804,\n",
       "   0.14643174409866333,\n",
       "   2.0801033973693848,\n",
       "   -0.5192757248878479,\n",
       "   0.0034178541973233223,\n",
       "   -0.8312557339668274,\n",
       "   0.7083359956741333,\n",
       "   0.4190274178981781,\n",
       "   -0.24384663999080658,\n",
       "   -0.4381244480609894,\n",
       "   1.7234349250793457,\n",
       "   0.202565535902977,\n",
       "   0.39412304759025574,\n",
       "   0.5309081077575684,\n",
       "   -0.8102590441703796,\n",
       "   -1.1088725328445435,\n",
       "   -1.6162774562835693,\n",
       "   -0.2881922721862793,\n",
       "   0.3485404849052429,\n",
       "   -1.378768801689148,\n",
       "   -1.357771873474121,\n",
       "   -0.7928052544593811,\n",
       "   -1.4515414237976074,\n",
       "   1.294967532157898,\n",
       "   -0.8048083782196045,\n",
       "   0.1542646735906601,\n",
       "   0.5184518694877625,\n",
       "   -0.20302696526050568,\n",
       "   -0.451728492975235,\n",
       "   0.22871264815330505,\n",
       "   0.2727130055427551,\n",
       "   0.4468119442462921,\n",
       "   -0.8798182010650635,\n",
       "   -0.15626069903373718,\n",
       "   0.3294370472431183,\n",
       "   -0.12672537565231323,\n",
       "   -1.1084715127944946,\n",
       "   0.5769520401954651,\n",
       "   0.26031216979026794,\n",
       "   0.690294086933136,\n",
       "   -0.23039773106575012,\n",
       "   -1.4911134243011475,\n",
       "   -1.433470606803894,\n",
       "   0.042943019419908524,\n",
       "   3.1025543212890625,\n",
       "   -0.9263572692871094,\n",
       "   -1.2858831882476807,\n",
       "   -0.8045607805252075,\n",
       "   -1.0653563737869263,\n",
       "   -0.9057060480117798,\n",
       "   2.344980478286743,\n",
       "   -1.0267239809036255,\n",
       "   0.2864427864551544,\n",
       "   -1.578405499458313,\n",
       "   -0.457552045583725,\n",
       "   0.06129883974790573,\n",
       "   0.6023485660552979,\n",
       "   0.6338008642196655,\n",
       "   2.097080707550049,\n",
       "   -1.0926177501678467,\n",
       "   -0.9363697171211243,\n",
       "   1.88141667842865,\n",
       "   1.6523287296295166,\n",
       "   -0.8713611364364624,\n",
       "   1.092647671699524,\n",
       "   0.01830233447253704,\n",
       "   -2.656179428100586,\n",
       "   1.9083274602890015,\n",
       "   0.7399221062660217,\n",
       "   0.4293980896472931,\n",
       "   -0.31313976645469666,\n",
       "   0.982511043548584,\n",
       "   -1.4040493965148926,\n",
       "   -2.3592123985290527,\n",
       "   -0.89549320936203,\n",
       "   -0.08697288483381271,\n",
       "   -0.5998033881187439,\n",
       "   0.7180025577545166,\n",
       "   -0.036790505051612854,\n",
       "   0.47990310192108154,\n",
       "   -0.37561285495758057,\n",
       "   0.31540241837501526,\n",
       "   -0.6587954759597778,\n",
       "   -1.114748239517212,\n",
       "   0.8093197345733643,\n",
       "   -0.66845703125,\n",
       "   0.6574963927268982,\n",
       "   1.4495487213134766,\n",
       "   -0.49251118302345276,\n",
       "   0.08004891872406006,\n",
       "   -0.2168125957250595,\n",
       "   -0.10062142461538315,\n",
       "   0.8941659927368164,\n",
       "   1.052611231803894,\n",
       "   -3.0644054412841797,\n",
       "   0.2747143805027008,\n",
       "   -0.4068167507648468,\n",
       "   1.0771301984786987,\n",
       "   -0.20231854915618896,\n",
       "   0.11654210090637207,\n",
       "   -1.4201748371124268,\n",
       "   -1.07484769821167,\n",
       "   -0.739771842956543,\n",
       "   -0.9014240503311157,\n",
       "   -0.5379541516304016,\n",
       "   0.6571444272994995,\n",
       "   0.35767585039138794,\n",
       "   0.8283942937850952,\n",
       "   1.150254249572754,\n",
       "   -1.6378042697906494,\n",
       "   1.9535479545593262,\n",
       "   1.7694748640060425,\n",
       "   -0.11907140165567398,\n",
       "   -1.6875059604644775,\n",
       "   -0.6611285209655762,\n",
       "   0.2548261284828186,\n",
       "   1.1182633638381958,\n",
       "   1.519927978515625,\n",
       "   -0.7269613742828369,\n",
       "   0.6565043926239014,\n",
       "   0.28053444623947144,\n",
       "   1.640343427658081,\n",
       "   -0.2902611792087555,\n",
       "   1.4057221412658691,\n",
       "   0.692223310470581,\n",
       "   0.2885580062866211,\n",
       "   -0.060435451567173004,\n",
       "   -1.0818122625350952,\n",
       "   0.16344502568244934,\n",
       "   0.7171393632888794,\n",
       "   -0.22370126843452454,\n",
       "   -0.5909973382949829,\n",
       "   -0.4629610776901245,\n",
       "   1.2553476095199585,\n",
       "   -0.8535116910934448,\n",
       "   -1.604320764541626,\n",
       "   0.8412861824035645,\n",
       "   1.5546256303787231,\n",
       "   -0.16237375140190125,\n",
       "   0.8293609619140625,\n",
       "   0.8109762668609619,\n",
       "   -0.028638280928134918,\n",
       "   -2.0508296489715576,\n",
       "   0.5806353688240051,\n",
       "   -1.850982904434204,\n",
       "   0.578827440738678,\n",
       "   0.7787002921104431,\n",
       "   -0.03269661217927933,\n",
       "   -0.8176304697990417,\n",
       "   0.8019407987594604,\n",
       "   -0.4152579605579376,\n",
       "   -0.6449868083000183,\n",
       "   -1.7205095291137695,\n",
       "   -0.20859062671661377,\n",
       "   -2.4485280513763428,\n",
       "   1.7669867277145386,\n",
       "   0.6587371230125427,\n",
       "   0.2624083161354065,\n",
       "   0.6952924728393555,\n",
       "   -1.0075024366378784,\n",
       "   0.8310197591781616,\n",
       "   0.18093428015708923,\n",
       "   -0.16947077214717865,\n",
       "   -0.6192375421524048,\n",
       "   0.5405148267745972,\n",
       "   0.6595795154571533,\n",
       "   -0.3859204351902008,\n",
       "   2.446230173110962,\n",
       "   0.7414036989212036,\n",
       "   -1.799700140953064,\n",
       "   0.26206153631210327,\n",
       "   0.6566714644432068,\n",
       "   -0.7911229133605957,\n",
       "   -0.1289462000131607,\n",
       "   1.3831325769424438,\n",
       "   -0.7663050889968872,\n",
       "   0.6331496834754944,\n",
       "   -0.1825127899646759,\n",
       "   1.105117678642273,\n",
       "   0.6921559572219849,\n",
       "   2.4002442359924316,\n",
       "   0.34809041023254395,\n",
       "   -0.5728098750114441,\n",
       "   0.16873252391815186,\n",
       "   -0.7813729643821716,\n",
       "   0.5408951640129089,\n",
       "   1.5118610858917236,\n",
       "   -0.8308563232421875,\n",
       "   0.24748364090919495,\n",
       "   0.33261018991470337,\n",
       "   -1.3928096294403076,\n",
       "   0.43809545040130615,\n",
       "   0.3409310579299927,\n",
       "   0.3906703591346741,\n",
       "   -0.5897515416145325,\n",
       "   -0.7194406986236572,\n",
       "   0.6588767766952515,\n",
       "   0.23170073330402374,\n",
       "   0.43914690613746643,\n",
       "   1.4431302547454834,\n",
       "   0.5705267786979675,\n",
       "   1.3734569549560547,\n",
       "   -0.42504820227622986,\n",
       "   0.850599467754364,\n",
       "   -0.7810952067375183,\n",
       "   -1.661779761314392,\n",
       "   0.19465330243110657,\n",
       "   0.6199372410774231,\n",
       "   -0.8459482789039612,\n",
       "   -1.0876438617706299,\n",
       "   0.2518523037433624,\n",
       "   1.642801284790039,\n",
       "   -2.0683932304382324,\n",
       "   1.1152361631393433,\n",
       "   0.613945484161377,\n",
       "   -0.2217654287815094,\n",
       "   -1.4017720222473145,\n",
       "   -0.39720043540000916,\n",
       "   -0.00851539708673954,\n",
       "   0.5995046496391296,\n",
       "   0.448426216840744,\n",
       "   2.6579015254974365,\n",
       "   0.30706968903541565,\n",
       "   0.16678786277770996,\n",
       "   0.5868529677391052,\n",
       "   1.2824755907058716,\n",
       "   -2.309992790222168,\n",
       "   -1.0222020149230957,\n",
       "   1.1353683471679688,\n",
       "   1.213244915008545,\n",
       "   0.39527320861816406,\n",
       "   -2.121108293533325,\n",
       "   -0.8093869090080261,\n",
       "   0.6400124430656433,\n",
       "   -0.3028227388858795,\n",
       "   -1.269148349761963,\n",
       "   1.2230288982391357,\n",
       "   -0.14555302262306213,\n",
       "   1.8018596172332764,\n",
       "   -0.8748810291290283,\n",
       "   -0.7568293809890747,\n",
       "   -2.2445268630981445,\n",
       "   -0.7006885409355164,\n",
       "   0.3851832449436188,\n",
       "   -0.0980260819196701,\n",
       "   -0.006170434877276421,\n",
       "   0.23351451754570007,\n",
       "   -0.5159852504730225,\n",
       "   0.8462932109832764,\n",
       "   0.007986568845808506,\n",
       "   1.797399640083313,\n",
       "   0.9454957842826843,\n",
       "   -0.30372539162635803,\n",
       "   -0.9125429391860962,\n",
       "   0.03345508873462677,\n",
       "   1.6216710805892944,\n",
       "   -2.1679329872131348,\n",
       "   -1.2250337600708008,\n",
       "   -2.4696199893951416,\n",
       "   -1.3115774393081665,\n",
       "   -1.4043647050857544,\n",
       "   0.14092175662517548,\n",
       "   1.3150511980056763,\n",
       "   0.2177075445652008,\n",
       "   -1.3087002038955688,\n",
       "   0.15476632118225098,\n",
       "   -0.274686336517334,\n",
       "   -1.5266227722167969,\n",
       "   -0.09316567331552505,\n",
       "   -0.4719439744949341,\n",
       "   -0.14666128158569336,\n",
       "   -0.30572107434272766,\n",
       "   -0.5017948746681213,\n",
       "   -1.379844307899475,\n",
       "   0.6909616589546204,\n",
       "   -0.18804030120372772,\n",
       "   -0.09910772740840912,\n",
       "   0.590995728969574,\n",
       "   0.7928629517555237,\n",
       "   -0.06647175550460815,\n",
       "   0.8461035490036011,\n",
       "   -0.9509612917900085,\n",
       "   -0.28702792525291443,\n",
       "   0.19237451255321503,\n",
       "   -0.24863965809345245,\n",
       "   -0.7207531929016113,\n",
       "   -0.44456976652145386,\n",
       "   1.5823767185211182,\n",
       "   0.8617451190948486,\n",
       "   -0.34160175919532776,\n",
       "   2.1678433418273926,\n",
       "   2.0305774211883545,\n",
       "   1.8219170570373535,\n",
       "   -0.007105240598320961,\n",
       "   0.7020973563194275,\n",
       "   -1.026991605758667,\n",
       "   0.22242844104766846,\n",
       "   -1.097025752067566,\n",
       "   1.0236048698425293,\n",
       "   0.9144180417060852,\n",
       "   -0.18412257730960846,\n",
       "   0.7983263731002808,\n",
       "   0.37377315759658813,\n",
       "   0.9564680457115173,\n",
       "   0.9977599382400513,\n",
       "   -0.8333251476287842,\n",
       "   0.4350506365299225,\n",
       "   0.1303478479385376,\n",
       "   -0.3406805396080017,\n",
       "   1.3102396726608276,\n",
       "   -0.4865870177745819,\n",
       "   0.7453047037124634,\n",
       "   0.6030164361000061,\n",
       "   0.3528975546360016,\n",
       "   -0.6638209223747253,\n",
       "   -1.0620805025100708,\n",
       "   1.2534335851669312,\n",
       "   -0.6817850470542908,\n",
       "   -1.4263967275619507,\n",
       "   -0.7937646508216858,\n",
       "   1.773858904838562,\n",
       "   -0.3844050168991089,\n",
       "   -0.14655299484729767,\n",
       "   4.483913421630859,\n",
       "   0.08532144129276276,\n",
       "   -1.0534766912460327,\n",
       "   -0.7015476226806641,\n",
       "   -0.320769727230072,\n",
       "   0.977933406829834,\n",
       "   -0.3155796229839325,\n",
       "   -0.03511251509189606,\n",
       "   1.5319759845733643,\n",
       "   -0.3074360191822052,\n",
       "   -0.24884726107120514,\n",
       "   -0.7024893760681152,\n",
       "   0.6670616269111633,\n",
       "   1.0726583003997803,\n",
       "   1.8504735231399536,\n",
       "   -0.3605377674102783,\n",
       "   -0.4489312469959259,\n",
       "   0.16247624158859253,\n",
       "   -0.19820040464401245,\n",
       "   -0.13502953946590424,\n",
       "   0.12536929547786713,\n",
       "   0.8505539298057556,\n",
       "   -0.5391556024551392,\n",
       "   -0.25782305002212524,\n",
       "   0.11002737283706665,\n",
       "   1.5853657722473145,\n",
       "   0.6689270734786987,\n",
       "   -2.291404962539673,\n",
       "   1.1967644691467285,\n",
       "   -0.19753415882587433,\n",
       "   0.3743499517440796,\n",
       "   -1.6554960012435913,\n",
       "   0.33215999603271484,\n",
       "   -0.6192111968994141,\n",
       "   1.9653877019882202,\n",
       "   1.4717581272125244,\n",
       "   0.6940438747406006,\n",
       "   1.4894142150878906,\n",
       "   -1.60392165184021,\n",
       "   -0.7330631613731384,\n",
       "   0.22725266218185425,\n",
       "   -0.5773077011108398,\n",
       "   -0.4374617338180542,\n",
       "   -1.4971872568130493,\n",
       "   0.2256767749786377,\n",
       "   -0.7935850620269775,\n",
       "   -0.5406559109687805,\n",
       "   -0.5396713614463806,\n",
       "   -0.5697691440582275,\n",
       "   0.5955507159233093,\n",
       "   2.334390163421631,\n",
       "   -0.02676767110824585,\n",
       "   -1.0497254133224487,\n",
       "   -0.5063824653625488,\n",
       "   -1.2924164533615112,\n",
       "   -0.4014780819416046,\n",
       "   0.6208962202072144,\n",
       "   0.3103782832622528,\n",
       "   0.568596601486206,\n",
       "   -0.856438934803009,\n",
       "   0.18656477332115173,\n",
       "   1.4414187669754028,\n",
       "   1.6082264184951782,\n",
       "   -0.4923984408378601,\n",
       "   0.6263415813446045,\n",
       "   -0.08131130039691925,\n",
       "   0.5899709463119507,\n",
       "   -2.074385643005371,\n",
       "   -0.03139057010412216,\n",
       "   0.15161767601966858,\n",
       "   -0.2597978711128235,\n",
       "   -1.096731185913086,\n",
       "   0.4240601062774658,\n",
       "   1.6832196712493896,\n",
       "   -0.4925450384616852,\n",
       "   -1.7048118114471436,\n",
       "   -0.3235020637512207,\n",
       "   -0.17198437452316284,\n",
       "   0.3094826936721802,\n",
       "   -0.9082472920417786,\n",
       "   -0.185665562748909,\n",
       "   1.25973379611969,\n",
       "   -0.5185291171073914,\n",
       "   -1.8253062963485718,\n",
       "   -1.7895351648330688,\n",
       "   -0.21014751493930817,\n",
       "   -0.7240989804267883,\n",
       "   -0.5313976407051086,\n",
       "   -0.19142194092273712,\n",
       "   -1.74015474319458,\n",
       "   -1.8012224435806274,\n",
       "   0.13213789463043213,\n",
       "   -0.747173011302948,\n",
       "   -0.6795317530632019,\n",
       "   0.4656253159046173,\n",
       "   0.05115170031785965,\n",
       "   0.7362115383148193,\n",
       "   0.9609177708625793,\n",
       "   0.8314299583435059,\n",
       "   -1.0663628578186035,\n",
       "   1.3670849800109863,\n",
       "   0.19498753547668457,\n",
       "   0.9582219123840332,\n",
       "   0.04808574169874191,\n",
       "   0.08064272999763489,\n",
       "   0.7147423028945923,\n",
       "   -1.2768054008483887,\n",
       "   -1.0578378438949585,\n",
       "   0.2930302321910858,\n",
       "   -1.5681217908859253,\n",
       "   0.7532238960266113,\n",
       "   2.240490436553955,\n",
       "   1.633860468864441,\n",
       "   1.2456154823303223,\n",
       "   2.8640122413635254,\n",
       "   0.6739904284477234,\n",
       "   -0.5050857663154602,\n",
       "   -0.5515718460083008,\n",
       "   0.7441440224647522,\n",
       "   0.44794297218322754,\n",
       "   -1.1464526653289795,\n",
       "   0.18728040158748627,\n",
       "   0.027642151340842247,\n",
       "   -0.6707905530929565,\n",
       "   -0.027005426585674286,\n",
       "   -0.6667246222496033,\n",
       "   -1.032496452331543,\n",
       "   -0.5461665391921997,\n",
       "   1.30652916431427,\n",
       "   -0.9035961627960205,\n",
       "   1.156325340270996,\n",
       "   -0.17760756611824036,\n",
       "   -2.0957369804382324,\n",
       "   0.33613675832748413,\n",
       "   -0.5389829277992249,\n",
       "   -0.06609539687633514,\n",
       "   0.5366426706314087,\n",
       "   -0.9082661271095276,\n",
       "   0.39926663041114807,\n",
       "   -0.5247668623924255,\n",
       "   1.173100233078003,\n",
       "   -0.8399195671081543,\n",
       "   -0.3622725009918213,\n",
       "   0.5465753078460693,\n",
       "   -1.5933295488357544,\n",
       "   0.6612337231636047,\n",
       "   1.0465415716171265,\n",
       "   2.856788396835327,\n",
       "   -0.7315527200698853,\n",
       "   -0.263441264629364,\n",
       "   0.4393553137779236,\n",
       "   -0.3771125376224518,\n",
       "   -1.696367859840393,\n",
       "   0.99028080701828,\n",
       "   -1.5939399003982544,\n",
       "   0.450064480304718,\n",
       "   -1.6862131357192993,\n",
       "   1.3623968362808228,\n",
       "   0.23356996476650238,\n",
       "   0.054830946028232574,\n",
       "   1.4960800409317017,\n",
       "   1.4684672355651855,\n",
       "   -0.1073872298002243,\n",
       "   0.9104561805725098,\n",
       "   -0.5650107264518738],\n",
       "  'facial_area': {'x': 1438,\n",
       "   'y': 1869,\n",
       "   'w': 919,\n",
       "   'h': 1058,\n",
       "   'left_eye': (2134, 2242),\n",
       "   'right_eye': (1705, 2207)},\n",
       "  'face_confidence': 1.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi mom\n"
     ]
    }
   ],
   "source": [
    "print(\"hi mom\")\n",
    "import gc, torch, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpbansal/code/ubuntu_img_search/img_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpbansal/code/ubuntu_img_search/img_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer,\n",
    "                           BertForMultipleChoice, BertModel, BertConfig, RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                           T5Tokenizer, AutoTokenizer, LlamaTokenizerFast, GemmaTokenizerFast, Gemma2Config, Gemma2ForSequenceClassification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}\n",
      "{0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bert = BertConfig.from_pretrained(\"bert-large-cased\", classifier_dropout=0.3, problem_type=\"multi_label_classification\",\n",
    "                                    num_labels=3, id2label=id2label, label2id=label2id)\n",
    "config_Roberta = RobertaConfig.from_pretrained(\"roberta-large\", classifier_dropout=0.3, problem_type=\"multi_label_classification\",\n",
    "                                    num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "config_gemma = Gemma2Config.from_pretrained(\"google/gemma-2-9b\", classifier_dropout=0.25, problem_type=\"multi_label_classification\",\n",
    "                                            num_labels=3, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, config):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                                     config\u001b[38;5;241m=\u001b[39mconfig_bert\n\u001b[1;32m      3\u001b[0m                                                     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RobertaForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                    config=config_bert\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-3b\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-base-zh\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet/xlnet-large-cased\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/arpbansal/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
    "\"\"\"PADDING TOKEN NOT USED IN LLAMA 3\n",
    "    TOKEN_TYPE_IDS NOT AVAILABLE on DEFAULT\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PADDING TOKEN WORKS IN GEMMA 2\\n    TOKEN_TYPE_IDS NOT AVAILABLE on DEFAULT'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-2-9b\")\n",
    "\"\"\"PADDING TOKEN WORKS IN GEMMA 2\n",
    "    TOKEN_TYPE_IDS NOT AVAILABLE on DEFAULT\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm2_5-7b\", trust_remote_code=True, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1= \"hi hello mom\"\n",
    "sen2= \"namaste ji papa\"\n",
    "tes_1='prompt:[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]\\nresponse_model_b:[\"As an AI, I don\\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\\'s a topic with valid arguments on both sides, and it\\'s up to each individual or organization to decide what they believe is right.\",\"As an AI, I don\\'t eat, so I don\\'t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It\\'s a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"]'\n",
    "tes_2 ='prompt:[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]\\nresponse_model_b:[\"As an AI, I don\\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\\'s a topic with valid arguments on both sides, and it\\'s up to each individual or organization to decide what they believe is right.\",\"As an AI, I don\\'t eat, so I don\\'t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It\\'s a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-large-cased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models\n",
    "    XLnet, ERNIE\n",
    "\n",
    "    t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_a = tokenizer(sen1, padding=True, max_length=2048, truncation=True, return_tensors=\"pt\")\n",
    "encoded_b = tokenizer(sen2, padding=True, max_length=2048, truncation=True, return_tensors=\"pt\")\n",
    "encoded_ab= tokenizer(sen1, sen2, padding=True, max_length=2048, truncation=True, return_tensors=\"pt\",\n",
    "                        add_special_tokens=True, return_token_type_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "labels=(1,  0, 0,  0, 0,  0)\n",
    "LABELS=['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "# labels=sample[LABELS]\n",
    "# labels=['antagonize', 'condescending','dismissive','generalisation',\n",
    "            #   'generalisation_unfair','hostile','sarcastic','unhealthy']\n",
    "labels=torch.FloatTensor(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(sample):\n",
    "    a=\"winner_model_a\"\n",
    "    b=\"winner_model_b\"\n",
    "    t=\"winner_tie\"\n",
    "    if sample[a] == 1:\n",
    "        return {\"labels\": f\"{a}\"}\n",
    "    elif sample[b] == 1:\n",
    "        return {\"labels\": f\"{b}\"}\n",
    "    else :\n",
    "        return {\"labels\": f\"{t}\"}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_1 = tokenizer(tes_1, tes_2, padding=True, max_length=2048, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,  39038, 235292,   3681,   2437,    665,  89397,   1833,    577,\n",
       "           3418,    577,    791,    476,   3383,  12070,    576,  27751,    611,\n",
       "          83754,  12568, 235336,   8332,   4083, 235269,   1721,  62576,   8070,\n",
       "            611,    476,  14967, 235336,  52591,    578,   2734,    682,   2245,\n",
       "           3448, 163394,    108,   4250, 235298,   2516, 235298, 235268, 235292,\n",
       "           3681,   2169,    671,  16481, 235269,    590,   1453, 235303, 235251,\n",
       "            791,   3749,  29013,    689,  18475, 235265,   4560, 235269,    590,\n",
       "            798,   3337,    692,    674,    573,   2872,    576,  15408, 109538,\n",
       "            575,  83754,  12568,    603,    476,   5766,    974,    578,  12568,\n",
       "            798,  11633, 235265, 177307,  24930,    674,   1582,   8292,    798,\n",
       "           1707,   5112,  13457, 165531, 235269,  14650,  19871, 235269,    578,\n",
       "           1249,   2389,    577,   2525,   4530, 235290,  14577, 235265, 109776,\n",
       "           1249,  24930,    674,  12568,   1412,    614,   3482,  31459,    611,\n",
       "          30805, 235269,    578,    674, 109538,   1538,  21145,   2389,    577,\n",
       "           8447,   2285,    689, 140759,  17725,   1855,  33693, 235265, 196568,\n",
       "            798,    614,  45927,    578,  42365,    774,   1552,    577,   1552,\n",
       "           3482,    611,   1024,   3749,  29013, 235269,  10831,   3824, 235269,\n",
       "            578,   4035, 235265,   1165, 235303, 235256,    476,  11381,    675,\n",
       "           4819,  12558,    611,   2145,  10012, 235269,    578,    665, 235303,\n",
       "         235256,    908,    577,   1853,   3811,    689,   8344,    577,  11958,\n",
       "           1212,    984,   4564,    603,   1833, 235265,   8332,   2169,    671,\n",
       "          16481, 235269,    590,   1453, 235303, 235251,   7812, 235269,    712,\n",
       "            590,   1453, 235303, 235251,    791,   3749,  22733, 235265,   2005,\n",
       "         235269,    575,    573,   2134,    576,  14967, 235269,    573,  62576,\n",
       "          73881,    603,    476, 216449,  98177,  11381, 235341,   4213,   1461,\n",
       "           2182,    573,   7786,    578, 132968,   7345,    665,  14042,    577,\n",
       "            476,  14967, 235269,    476,  22150,  11137,   1644,  10964,    731,\n",
       "            573,  65018,  14967, 235265,  25481,  31770,   4564,    674,    573,\n",
       "          73475,    576,    573,   9471,    919,    793,   2040,    611,    476,\n",
       "          14967, 235265,   1165, 235303, 235256,    476,  14967,  24754,    674,\n",
       "           2613,   2447,    614,  23895, 235265,   1704, 235269,   4270,  62576,\n",
       "          17239,    611,    476,  14967,    689,    780, 235269,    665,  11135,\n",
       "          12014,    611,    861,  10122,  45951, 235341,   4437,      2,  39038,\n",
       "         235292,   3681,   2437,    665,  89397,   1833,    577,   3418,    577,\n",
       "            791,    476,   3383,  12070,    576,  27751,    611,  83754,  12568,\n",
       "         235336,   8332,   4083, 235269,   1721,  62576,   8070,    611,    476,\n",
       "          14967, 235336,  52591,    578,   2734,    682,   2245,   3448, 163394,\n",
       "            108,   4250, 235298,   2516, 235298, 235268, 235292,   3681,   2169,\n",
       "            671,  16481, 235269,    590,   1453, 235303, 235251,    791,   3749,\n",
       "          29013,    689,  18475, 235265,   4560, 235269,    590,    798,   3337,\n",
       "            692,    674,    573,   2872,    576,  15408, 109538,    575,  83754,\n",
       "          12568,    603,    476,   5766,    974,    578,  12568,    798,  11633,\n",
       "         235265, 177307,  24930,    674,   1582,   8292,    798,   1707,   5112,\n",
       "          13457, 165531, 235269,  14650,  19871, 235269,    578,   1249,   2389,\n",
       "            577,   2525,   4530, 235290,  14577, 235265, 109776,   1249,  24930,\n",
       "            674,  12568,   1412,    614,   3482,  31459,    611,  30805, 235269,\n",
       "            578,    674, 109538,   1538,  21145,   2389,    577,   8447,   2285,\n",
       "            689, 140759,  17725,   1855,  33693, 235265, 196568,    798,    614,\n",
       "          45927,    578,  42365,    774,   1552,    577,   1552,   3482,    611,\n",
       "           1024,   3749,  29013, 235269,  10831,   3824, 235269,    578,   4035,\n",
       "         235265,   1165, 235303, 235256,    476,  11381,    675,   4819,  12558,\n",
       "            611,   2145,  10012, 235269,    578,    665, 235303, 235256,    908,\n",
       "            577,   1853,   3811,    689,   8344,    577,  11958,   1212,    984,\n",
       "           4564,    603,   1833, 235265,   8332,   2169,    671,  16481, 235269,\n",
       "            590,   1453, 235303, 235251,   7812, 235269,    712,    590,   1453,\n",
       "         235303, 235251,    791,   3749,  22733, 235265,   2005, 235269,    575,\n",
       "            573,   2134,    576,  14967, 235269,    573,  62576,  73881,    603,\n",
       "            476, 216449,  98177,  11381, 235341,   4213,   1461,   2182,    573,\n",
       "           7786,    578, 132968,   7345,    665,  14042,    577,    476,  14967,\n",
       "         235269,    476,  22150,  11137,   1644,  10964,    731,    573,  65018,\n",
       "          14967, 235265,  25481,  31770,   4564,    674,    573,  73475,    576,\n",
       "            573,   9471,    919,    793,   2040,    611,    476,  14967, 235265,\n",
       "           1165, 235303, 235256,    476,  14967,  24754,    674,   2613,   2447,\n",
       "            614,  23895, 235265,   1704, 235269,   4270,  62576,  17239,    611,\n",
       "            476,  14967,    689,    780, 235269,    665,  11135,  12014,    611,\n",
       "            861,  10122,  45951, 235341,   4437]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "encoded_2 = tokenizer([tes_1, tes_2], padding=True, truncation=True, return_tensors=\"pt\")  # Seems more appropriate approach for now w.r.t other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_encoding={}\n",
    "for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "    if key in encoded_a and key in encoded_b:\n",
    "        concat_encoding[key] = torch.cat([encoded_a[key], encoded_b[key]], dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,   544, 25612,  3278,     2,   556,  2616,   488, 22425, 33262]]),\n",
       " tensor([    2,   544, 25612,  3278,     2,   556,  2616,   488, 22425, 33262]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ab['input_ids'], encoded_ab['input_ids'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  6176, 24176,  3532]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding={}\n",
    "# for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "#     if key in encoded_a_b:\n",
    "#         encoding[key] = torch.cat([encoded_a_b[key][0], encoded_a_b[key][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7632,  7592,   102,     0,     0],\n",
       "        [  101, 15125, 14083,  2063, 10147,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([sen1, sen2], padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Gemma2ForSequenceClassification.from_pretrained(\"google/gemma-2-9b\", config=config_gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            prompt:            response_a            response_b  \\\n",
      "0  This is prompt 1  This is response A 1  This is response B 1   \n",
      "1  This is prompt 2  This is response A 2  This is response B 2   \n",
      "\n",
      "                                                text  \n",
      "0  prompt:This is prompt 1, response_a:This is re...  \n",
      "1  prompt:This is prompt 2, response_a:This is re...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (adjust column names if needed)\n",
    "data = {\n",
    "    \"prompt:\": [\"This is prompt 1\", \"This is prompt 2\"],\n",
    "    \"response_a\": [\"This is response A 1\", \"This is response A 2\"],\n",
    "    \"response_b\": [\"This is response B 1\", \"This is response B 2\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def combine_columns(row):\n",
    "  # Use f-string formatting for desired structure\n",
    "  return f\"prompt:{row['prompt:']}, response_a:{row['response_a']}, response_b:{row['response_b']}\"\n",
    "\n",
    "# Apply combine function\n",
    "df[\"text\"] = df.apply(combine_columns, axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt:', 'response_a', 'response_b', 'text'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prompt:This is prompt 1, response_a:This is response A 1, response_b:This is response B 1',\n",
       " 'prompt:This is prompt 2, response_a:This is response A 2, response_b:This is response B 2']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3282"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "    \"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "    assert(isinstance(size, torch.Size))\n",
    "    return \"  \".join(map(str, size))\n",
    "def dump_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "    import gc\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "                                          \" GPU\" if obj.is_cuda else \"\",\n",
    "                                          \" pinned\" if obj.is_pinned() else \"\",\n",
    "                                          pretty_size(obj.size())))\n",
    "                    total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s  %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "                                                   type(obj.data).__name__, \n",
    "                                                   \" GPU\" if obj.is_cuda else \"\",\n",
    "                                                   \" pinned\" if obj.data.is_pinned() else \"\",\n",
    "                                                   \" grad\" if obj.requires_grad else \"\", \n",
    "                                                   \" volatile\" if obj.volatile else \"\",\n",
    "                                                   pretty_size(obj.data.size())))\n",
    "                    total_size += obj.data.numel()\n",
    "        except Exception as e:\n",
    "            pass        \n",
    "    print(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpbansal/code/ubuntu_img_search/img_venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU 28996  1024\n",
      "Parameter: GPU 512  1024\n",
      "Parameter: GPU 2  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Tensor: GPU 1  512\n",
      "Tensor: GPU 1  512\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 3  768\n",
      "Parameter: GPU 3\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 4096  1024\n",
      "Parameter: GPU 4096\n",
      "Parameter: GPU 1024  4096\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024  1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Parameter: GPU 1024\n",
      "Total size: 333582595\n"
     ]
    }
   ],
   "source": [
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(row):\n",
    "    row[\"encode_fail\"] = False\n",
    "    try:\n",
    "        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except:\n",
    "        prompt = \"\"\n",
    "        row[\"encode_fail\"] = True\n",
    "\n",
    "    try:\n",
    "        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except:\n",
    "        response_a = \"\"\n",
    "        row[\"encode_fail\"] = True\n",
    "\n",
    "    try:\n",
    "        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except:\n",
    "        response_b = \"\"\n",
    "        row[\"encode_fail\"] = True\n",
    "        \n",
    "    # row['options'] = [f\"Prompt: {prompt}\\n\\nResponse: {response_a}\",  # Response from Model A\n",
    "    #                   f\"Prompt: {prompt}\\n\\nResponse: {response_b}\"  # Response from Model B\n",
    "    #                  ]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.2, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained('bert-large-cased')\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "device='cuda'\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.2, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_1(sample):\n",
    "    return {\"text\":f\"\"\"prompt:{sample['prompt']}\\nresponse_model_a:{sample['response_a']},\\nprompt:{sample['prompt']}response_model_b:{sample['response_b']}\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_2(sample):\n",
    "    return {\"text\":f\"\"\"prompt:{sample['prompt']}\\nresponse_model_a:{sample['response_a']},response_model_b:{sample['response_b']}\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def encode(sample):\n",
    "    text=sample['text']\n",
    "    encoding = tokenizer(text, truncation=True, max_length=512, padding=True, return_tensors='pt')\n",
    "    labels_batch = {k: sample[k] for k in sample.keys() if k in labels}\n",
    "      # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size):\n",
    "    gen_winner_a=[]\n",
    "    gen_winner_b=[]\n",
    "    gen_winner_tie=[]\n",
    "\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx=min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids=input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask=attention_mask[start_idx:end_idx].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # with autocast():\n",
    "                outputs = model(**inputs)\n",
    "        probabilities = torch.softmax(outputs, dim=-1).cpu().numpy()\n",
    "        gen_winner_a.extend(probabilities[:, 0])\n",
    "        gen_winner_b.extend(probabilities[:, 1])\n",
    "        gen_winner_tie.extend(probabilities[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(optimizer, batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lrfn)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'param_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lr_cb \u001b[38;5;241m=\u001b[39m \u001b[43mget_lr_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mget_lr_callback\u001b[0;34m(optimizer, batch_size, mode, epochs, plot)\u001b[0m\n\u001b[1;32m     20\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR Scheduler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambdaLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlrfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/ubuntu_img_search/img_venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:222\u001b[0m, in \u001b[0;36mLambdaLR.__init__\u001b[0;34m(self, optimizer, lr_lambda, last_epoch, verbose)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr_lambda, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr_lambda, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_lambdas \u001b[38;5;241m=\u001b[39m [lr_lambda] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lr_lambda) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mparam_groups):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'param_groups'"
     ]
    }
   ],
   "source": [
    "lr_cb = get_lr_callback(optimizer=optimizer, batch_size=16, plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(tokenizer, sample):\n",
    "    encoded = tokenizer(sample['conv_a'], sample['conv_b'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
